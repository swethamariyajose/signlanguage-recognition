## ABSTRACT

### COMPARATIVE STUDY OF DEEP LEARNING MODELS FOR SIGN LANGUAGE RECOGNITION

Sign language is a vital means of communication for the deaf and hard-of-hearing community, yet it remains largely underutilized due to limited understanding among the general population. This project aims to bridge this gap by developing a desktop-based application for real-time sign language recognition using deep learning models. The system integrates three state-of-the-art convolutional neural network architectures—InceptionResNetV2, ResNet50, and MobileNetV2—known for their efficiency and accuracy in image classification tasks.

The application is built using Python and Tkinter to provide an intuitive and user-friendly interface. Users can interact with the application to recognize and translate sign language gestures captured via a webcam into text, facilitating seamless communication. Each model is evaluated for its performance in terms of accuracy, speed, and resource utilization, ensuring the system operates efficiently on standard hardware.

This project highlights the potential of artificial intelligence in fostering inclusivity and demonstrates a comparative analysis of deep learning architectures for sign language recognition. The application not only serves as a communication tool but also sets the stage for further research and development in accessible technologies.
